{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning:\n",
    "Examples of Unsupervised and Supervised Machine-Learning Algorithms \n",
    "========\n",
    "\n",
    "##### Version 0.1\n",
    "\n",
    "Broadly speaking, machine-learning methods constitute a diverse collection of data-driven algorithms designed to classify/characterize/analyze sources in multi-dimensional spaces. The topics and studies that fall under the umbrella of machine learning is growing, and there is no good catch-all definition. The number (and variation) of algorithms is vast, and beyond the scope of these exercises. While we will discuss a few specific algorithms today, more importantly, we will explore the scope of the two general methods: unsupervised learning and supervised learning and introduce the powerful (and dangerous?) Python package [`scikit-learn`](http://scikit-learn.org/stable/).\n",
    "\n",
    "***\n",
    "By AA Miller\n",
    "\n",
    "2017 September 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1) Introduction to `scikit-learn`\n",
    "\n",
    "At the most basic level, `scikit-learn` makes machine learning extremely easy within `python`. By way of example, here is a short piece of code that builds a complex, non-linear model to classify sources in the Iris data set that we learned about earlier:\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    iris = datasets.load_iris()\n",
    "    RFclf = RandomForestClassifier().fit(iris.data, iris.target)\n",
    "\n",
    "Those 4 lines of code have constructed a model that is superior to any system of hard cuts that we could have encoded while looking at the multidimensional space. This can be fast as well: execute the dummy code in the cell below to see how \"easy\" machine-learning is with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute dummy code here\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "iris = datasets.load_iris()\n",
    "RFclf = RandomForestClassifier().fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, the procedure for `scikit-learn` is uniform across all machine-learning algorithms. Models are accessed via the various modules (`ensemble`, `SVM`, `neighbors`, etc), with user-defined tuning parameters. The features (or data) for the models are stored in a 2D array, `X`, with rows representing individual sources and columns representing the corresponding feature values. [In a minority of cases, `X`, represents a similarity or distance matrix where each entry represents the distance to every other source in the data set.] In cases where there is a known classification or scalar value (typically supervised methods), this information is stored in a 1D array `y`. \n",
    "\n",
    "Unsupervised models are fit by calling `.fit(X)` and supervised models are fit by calling `.fit(X, y)`. In both cases, predictions for new observations, `Xnew`, can be obtained by calling `.predict(Xnew)`. Those are the basics and beyond that, the details are algorithm specific, but the documentation for essentially everything within `scikit-learn` is excellent, so read the docs.\n",
    "\n",
    "To further develop our intuition, we will now explore the Iris dataset a little further.\n",
    "\n",
    "**Problem 1a** What is the pythonic type of `iris`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris) # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You likely haven't encountered a `scikit-learn Bunch` before. It's functionality is essentially the same as a dictionary. \n",
    "\n",
    "**Problem 1b** What are the keys of iris?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, iris contains `data` and `target` values. These are all you need for `scikit-learn`, though the feature and target names and description are useful.\n",
    "\n",
    "**Problem 1c** What is the shape and content of the `iris` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(iris['data'])) # complete\n",
    "#print(iris['data'])# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1d** What is the shape and content of the `iris` target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[ 5.1  4.9  4.7  4.6  5.   5.4  4.6  5.   4.4  4.9  5.4  4.8  4.8  4.3  5.8\n",
      "  5.7  5.4  5.1  5.7  5.1  5.4  5.1  4.6  5.1  4.8  5.   5.   5.2  5.2  4.7\n",
      "  4.8  5.4  5.2  5.5  4.9  5.   5.5  4.9  4.4  5.1  5.   4.5  4.4  5.   5.1\n",
      "  4.8  5.1  4.6  5.3  5.   7.   6.4  6.9  5.5  6.5  5.7  6.3  4.9  6.6  5.2\n",
      "  5.   5.9  6.   6.1  5.6  6.7  5.6  5.8  6.2  5.6  5.9  6.1  6.3  6.1  6.4\n",
      "  6.6  6.8  6.7  6.   5.7  5.5  5.5  5.8  6.   5.4  6.   6.7  6.3  5.6  5.5\n",
      "  5.5  6.1  5.8  5.   5.6  5.7  5.7  6.2  5.1  5.7  6.3  5.8  7.1  6.3  6.5\n",
      "  7.6  4.9  7.3  6.7  7.2  6.5  6.4  6.8  5.7  5.8  6.4  6.5  7.7  7.7  6.\n",
      "  6.9  5.6  7.7  6.3  6.7  7.2  6.2  6.1  6.4  7.2  7.4  7.9  6.4  6.3  6.1\n",
      "  7.7  6.3  6.4  6.   6.9  6.7  6.9  5.8  6.8  6.7  6.7  6.3  6.5  6.2  5.9]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(iris['target_names'])) # complete\n",
    "print(iris['feature_names'])# complete\n",
    "print(iris['data'][:,0])# complete\n",
    "print(iris['target_names'])# complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as a baseline for the exercises that follow, we will now make a simple 2D plot showing the separation of the 3 classes in the iris dataset. This plot will serve as the reference for examining the quality of the clustering algorithms. \n",
    "\n",
    "**Problem 1e** Make a scatter plot showing sepal length vs. sepal width for the iris data set. Color the points according to their respective classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc4ef070668>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuYuIVJCSu4hIBSm5i4hUkJK7iEgF/X9eBjCx2D+6ZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4ef1a2400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(iris.feature_names)  # shows that sepal length is first feature and sepal width is second feature\n",
    "x = iris['data'][:,0]\n",
    "y = iris['data'][:,1]\n",
    "plt.scatter(x[iris['target']==0],y[iris['target']==0],facecolor='red') # complete\n",
    "plt.scatter(x[iris['target']==1],y[iris['target']==1],facecolor='blue')# complete\n",
    "plt.scatter(x[iris['target']==2],y[iris['target']==2],facecolor='green')# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2) Unsupervised Machine Learning\n",
    "\n",
    "Unsupervised machine learning, sometimes referred to as clustering or data mining, aims to group or classify sources in the multidimensional feature space. The \"unsupervised\" comes from the fact that there are no target labels provided to the algorithm, so the machine is asked to cluster the data \"on its own.\" The lack of labels means there is no (simple) method for validating the accuracy of the solution provided by the machine (though sometimes simple examination can show the results are **terrible**). \n",
    "\n",
    "For this reason [*note* - this is my (AAM) opinion and there many be many others who disagree], unsupervised methods are not particularly useful for astronomy. Supposing one did find some useful clustering structure, an adversarial researcher could always claim that the current feature space does not accurately capture the physics of the system and as such the clustering result is not interesting or, worse, erroneous. The one potentially powerful exception to this broad statement is outlier detection, which can be a branch of both unsupervised and supervised learning. Finding weirdo objects is an astronomical pastime, and there are unsupervised methods that may help in that regard in the LSST era. \n",
    "\n",
    "To begin today we will examine one of the most famous, and simple, clustering algorithms: [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering). $k$-means clustering looks to identify $k$ convex clusters, where $k$ is a user defined number. And here-in lies the rub: if we truly knew the number of clusters in advance, we likely wouldn't need to perform any clustering in the first place. This is the major downside to $k$-means. Operationally, pseudocode for the algorithm can be summarized as the following: \n",
    "\n",
    "    initiate search by identifying k points (i.e. the cluster centers)\n",
    "    loop \n",
    "        assign each point in the data set to the closest cluster center\n",
    "        calculate new cluster centers based on mean position of all points within cluster\n",
    "        if diff(new center - old center) < threshold:\n",
    "            stop (i.e. clusters are defined)\n",
    "\n",
    "The threshold is defined by the user, though in some cases the total number of iterations is also. An advantage of $k$-means is that the solution will always converge, though the solution may only be a local minimum. Disadvantages include the assumption of convexity, i.e. difficult to capture complex geometry, and the curse of dimensionality.\n",
    "\n",
    "In `scikit-learn` the [`KMeans`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) algorithm is implemented as part of the [`sklearn.cluster`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster) module. \n",
    "\n",
    "**Problem 2a** Fit two different $k$-means models to the iris data, one with 2 clusters and one with 3 clusters. Plot the resulting clusters in the sepal length-sepal width plane (same plot as above). How do the results compare to the true classifications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc4ebaad048>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4ef0f7278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "Kcluster = KMeans(n_clusters=2) # complete\n",
    "Kcluster.fit(iris['data']) # complete\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(iris['data'][:,0],iris['data'][:,1]) # complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KMeans in module sklearn.cluster.k_means_:\n",
      "\n",
      "class KMeans(sklearn.base.BaseEstimator, sklearn.base.ClusterMixin, sklearn.base.TransformerMixin)\n",
      " |  K-Means clustering\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <k_means>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, optional, default: 8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  max_iter : int, default: 300\n",
      " |      Maximum number of iterations of the k-means algorithm for a\n",
      " |      single run.\n",
      " |  \n",
      " |  n_init : int, default: 10\n",
      " |      Number of time the k-means algorithm will be run with different\n",
      " |      centroid seeds. The final results will be the best output of\n",
      " |      n_init consecutive runs in terms of inertia.\n",
      " |  \n",
      " |  init : {'k-means++', 'random' or an ndarray}\n",
      " |      Method for initialization, defaults to 'k-means++':\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose k observations (rows) at random from data for\n",
      " |      the initial centroids.\n",
      " |  \n",
      " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |  algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\"\n",
      " |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      " |      The \"elkan\" variation is more efficient by using the triangle\n",
      " |      inequality, but currently doesn't support sparse data. \"auto\" chooses\n",
      " |      \"elkan\" for dense data and \"full\" for sparse data.\n",
      " |  \n",
      " |  precompute_distances : {'auto', True, False}\n",
      " |      Precompute distances (faster but takes more memory).\n",
      " |  \n",
      " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      " |      million. This corresponds to about 100MB overhead per job using\n",
      " |      double precision.\n",
      " |  \n",
      " |      True : always precompute distances\n",
      " |  \n",
      " |      False : never precompute distances\n",
      " |  \n",
      " |  tol : float, default: 1e-4\n",
      " |      Relative tolerance with regards to inertia to declare convergence\n",
      " |  \n",
      " |  n_jobs : int\n",
      " |      The number of jobs to use for the computation. This works by computing\n",
      " |      each of the n_init runs in parallel.\n",
      " |  \n",
      " |      If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
      " |      used at all, which is useful for debugging. For n_jobs below -1,\n",
      " |      (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
      " |      are used.\n",
      " |  \n",
      " |  random_state : integer or numpy.RandomState, optional\n",
      " |      The generator used to initialize the centers. If an integer is\n",
      " |      given, it fixes the seed. Defaults to the global numpy random\n",
      " |      number generator.\n",
      " |  \n",
      " |  verbose : int, default 0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  copy_x : boolean, default True\n",
      " |      When pre-computing distances it is more numerically accurate to center\n",
      " |      the data first.  If copy_x is True, then the original data is not\n",
      " |      modified.  If False, the original data is modified, and put back before\n",
      " |      the function returns, but small numerical differences may be introduced\n",
      " |      by subtracting and then adding the data mean.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cluster_centers_ : array, [n_clusters, n_features]\n",
      " |      Coordinates of cluster centers\n",
      " |  \n",
      " |  labels_ :\n",
      " |      Labels of each point\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      Sum of distances of samples to their closest cluster center.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from sklearn.cluster import KMeans\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      " |  ...               [4, 2], [4, 4], [4, 0]])\n",
      " |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      " |  >>> kmeans.labels_\n",
      " |  array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
      " |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      " |  array([0, 1], dtype=int32)\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[ 1.,  2.],\n",
      " |         [ 4.,  2.]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  \n",
      " |  MiniBatchKMeans\n",
      " |      Alternative online implementation that does incremental updates\n",
      " |      of the centers positions using mini-batches.\n",
      " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      " |      probably much faster than the default batch implementation.\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The k-means problem is solved using Lloyd's algorithm.\n",
      " |  \n",
      " |  The average complexity is given by O(k n T), were n is the number of\n",
      " |  samples and T is the number of iteration.\n",
      " |  \n",
      " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      " |  'How slow is the k-means method?' SoCG2006)\n",
      " |  \n",
      " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      " |  clustering algorithms available), but it falls in local minima. That's why\n",
      " |  it can be useful to restart it several times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KMeans\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute k-means clustering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Training instances to cluster.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X, y=None)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers.  Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape [n_samples, k]\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "help(KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3 clusters the algorithm does a good job of separating the three classes. However, without the a priori knowledge that there are 3 different types of iris, the 2 cluster solution would appear to be superior. \n",
    "\n",
    "**Problem 2b** How do the results change if the 3 cluster model is called with `n_init = 1` and `init = 'random'` options? Use `rs` for the random state [this allows me to cheat in service of making a point].\n",
    "\n",
    "*Note - the respective defaults for these two parameters are 10 and `k-means++`, respectively. Read the docs to see why these choices are, likely, better than those in 2b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = 14\n",
    "Kcluster1 = KMeans( # complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A random aside that is not particularly relevant here**\n",
    "\n",
    "$k$-means evaluates the Euclidean distance between individual sources and cluster centers, thus, the magnitude of the individual features has a strong effect on the final clustering outcome. \n",
    "\n",
    "**Problem 2c** Calculate the mean, standard deviation, min, and max of each feature in the iris data set. Based on these summaries, which feature is most important for clustering? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"feature\\t\\t\\tmean\\tstd\\tmin\\tmax\")\n",
    "for featnum, feat in enumerate(iris.feature_names):\n",
    "    print(\"{:s}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(feat, np.mean(iris.data[:,featnum]), \n",
    "                                                        np.std(iris.data[:,featnum]), np.min(iris.data[:,featnum]),\n",
    "                                                        np.max(iris.data[:,featnum])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petal length has the largest range and standard deviation, thus, it will have the most \"weight\" when determining the $k$ clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truth is that the iris data set is fairly small and straightfoward. Nevertheless, we will now examine the clustering results after re-scaling the features. [Some algorithms, *cough* Support Vector Machines *cough*, are notoriously sensitive to the feature scaling, so it is important to know about this step.] Imagine you are classifying stellar light curves: the data set will include contact binaries with periods of $\\sim 0.1 \\; \\mathrm{d}$ and Mira variables with periods of $\\gg 100 \\; \\mathrm{d}$. Without re-scaling, this feature that covers 4 orders of magnitude may dominate all others in the final model projections.\n",
    "\n",
    "The two most common forms of re-scaling are to rescale to a guassian with mean $= 0$ and variance $= 1$, or to rescale the min and max of the feature to $[0, 1]$. The best normalization is problem dependent. The [`sklearn.preprocessing`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module makes it easy to re-scale the feature set. **It is essential that the same scaling used for the training set be used for all other data run through the model.** The testing, validation, and field observations cannot be re-scaled independently. This would result in meaningless final classifications/predictions. \n",
    "\n",
    "**Problem 2d** Re-scale the features to normal distributions, and perform $k$-means clustering on the iris data. How do the results compare to those obtained earlier? \n",
    "\n",
    "*Hint - you may find [`'StandardScaler()'`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) within the `sklearn.preprocessing` module useful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit( # complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are almost identical to those obtained without scaling. This is due to the simplicity of the iris data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I test the accuracy of my clusters?**\n",
    "\n",
    "Essentially - you don't. There are some methods that are available, but they essentially compare clusters to labeled samples, and if the samples are labeled it is likely that supervised learning is more useful anyway. If you are curious, `scikit-learn` does provide some [built-in functions for analyzing clustering](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation), but again, it is difficult to evaluate the validity of any newly discovered clusters. \n",
    "\n",
    "**What if I don't know how many clusters are present in the data?**\n",
    "\n",
    "An excellent question, as you will almost never know this a priori. Many algorithms, like $k$-means, do require the number of clusters to be specified, but some other methods do not. As an example [`DBSCAN`](https://en.wikipedia.org/wiki/DBSCAN). In brief, `DBSCAN` requires two parameters: `minPts`, the minimum number of points necessary for a cluster, and $\\epsilon$, a distance measure. Clusters are grown by identifying *core points*, objects that have at least `minPts` located within a distance $\\epsilon$. *Reachable points* are those within a distance $\\epsilon$ of at least one *core point* but less than `minPts` *core points*. Identically, these points define the outskirts of the clusters. Finally, there are also *outliers* which are points that are $> \\epsilon$ away from any *core points*. Thus, `DBSCAN` naturally identifies clusters, does not assume clusters are convex, and even provides a notion of outliers. The downsides to the algorithm are that the results are highly dependent on the two tuning parameters, and that clusters of highly different densities can be difficult to recover (because $\\epsilon$ and `minPts` is specified for all clusters. \n",
    "\n",
    "In `scitkit-learn` the \n",
    "[`DBSCAN`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) algorithm is part of the `sklearn.cluster` module. $\\epsilon$ and `minPts` are set by `eps` and `min_samples`, respectively. \n",
    "\n",
    "**Problem 2e** Cluster the iris data using `DBSCAN`. Play around with the tuning parameters to see how they affect the final clustering results. How does the use of `DBSCAN` compare to $k$-means? Can you obtain 3 clusters with `DBSCAN`? If not, given the knowledge that the iris dataset has 3 classes - does this invalidate `DBSCAN` as a viable algorithm?\n",
    "\n",
    "*Note - DBSCAN labels outliers as $-1$, and thus, `plt.scatter()`, will plot all these points as the same color.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute this cell\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbs = DBSCAN(eps = 0.7, min_samples = 7)\n",
    "dbs.fit(scaler.transform(iris.data)) # best to use re-scaled data since eps is in absolute units\n",
    "\n",
    "dbs_outliers = dbs.labels_ == -1\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(iris.data[:,0], iris.data[:,1], c = dbs.labels_, s = 30, edgecolor = \"None\", cmap = \"viridis\")\n",
    "plt.scatter(iris.data[:,0][dbs_outliers], iris.data[:,1][dbs_outliers], s = 30, c = 'k')\n",
    "\n",
    "\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was unable to obtain 3 clusters with `DBSCAN`. While these results are, on the surface, worse than what we got with $k$-means, my suspicion is that the 4 features do not adequately separate the 3 classes. [See - a nayseyer can always make that argument.] This is not a problem for `DBSCAN` as an algorithm, but rather, evidence that no single algorithm works well in all cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem) Cluster SDSS Galaxy Data\n",
    "\n",
    "The following query will select 10k likely galaxies from the SDSS database and return the results of that query into an [`astropy.Table`](http://docs.astropy.org/en/stable/table/) object. (For now, if you are not familiar with the SDSS DB schema, don't worry about this query, just know that it returns a bunch of photometric features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from astroquery.sdss import SDSS  # enables direct queries to the SDSS database\n",
    "\n",
    "GALquery = \"\"\"SELECT TOP 10000 \n",
    "             p.dered_u - p.dered_g as ug, p.dered_g - p.dered_r as gr, \n",
    "             p.dered_g - p.dered_i as gi, p.dered_g - p.dered_z as gz,             \n",
    "             p.petroRad_i, p.petroR50_i, p.deVAB_i\n",
    "             FROM PhotoObjAll AS p JOIN specObjAll s ON s.bestobjid = p.objid\n",
    "             WHERE p.mode = 1 AND s.sciencePrimary = 1 AND p.clean = 1 AND p.type = 3\n",
    "               \"\"\"\n",
    "SDSSgals = SDSS.query_sql(GALquery)\n",
    "SDSSgals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used my own domain knowledge to specifically choose features that may be useful when clustering galaxies. If you know a bit about SDSS and can think of other features that may be useful feel free to add them to the query. \n",
    "\n",
    "One nice feature of `astropy` tables is that they can readily be turned into `pandas DataFrames`, which can in turn easily be turned into a `sklearn X` array with `NumPy`. For example: \n",
    "\n",
    "    X = np.array(SDSSgals.to_pandas())\n",
    "\n",
    "And you are ready to go. \n",
    "\n",
    "**Challenge Problem** Using the SDSS dataset above, identify interesting clusters within the data [this is intentionally very open ended, if you uncover anything especially exciting you'll have a chance to share it with the group]. Feel free to use the algorithms discussed above, or any other packages available via `sklearn`. Can you make sense of the clusters in the context of galaxy evolution? \n",
    "\n",
    "*Hint - don't fret if you know nothing about galaxy evolution (neither do I!). Just take a critical look at the clusters that are identified*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - I was unable to get the galaxies to clusster using DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 3) Supervised Machine Learning\n",
    "\n",
    "Supervised machine learning, on the other hand, aims to predict a target class or produce a regression result based on the location of labelled sources (i.e. the training set) in the multidimensional feature space. The \"supervised\" comes from the fact that we are specifying the allowed outputs from the model. As there are labels available for the training set, it is possible to estimate the accuracy of the model (though there are generally important caveats about generalization, which we will explore in further detail later).\n",
    "\n",
    "We will begin with a simple, but nevertheless, elegant algorithm for classification and regression: [$k$-nearest-neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) ($k$NN). In brief, the classification or regression output is determined by examining the $k$ nearest neighbors in the training set, where $k$ is a user defined number. Typically, though not always, distances between sources are Euclidean, and the final classification is assigned to whichever class has a plurality within the $k$ nearest neighbors (in the case of regression, the average of the $k$ neighbors is the output from the model). We will experiment with the steps necessary to optimize $k$, and other tuning parameters, in the detailed break-out problem.\n",
    "\n",
    "In `scikit-learn` the [`KNeighborsClassifer`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) algorithm is implemented as part of the [`sklearn.neighbors`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) module. \n",
    "\n",
    "**Problem 3a** \n",
    "\n",
    "Fit two different $k$NN models to the iris data, one with 3 neighbors and one with 10 neighbors. Plot the resulting class predictions in the sepal length-sepal width plane (same plot as above). How do the results compare to the true classifications? Is there any reason to be suspect of this procedure?\n",
    "\n",
    "*Hint - after you have constructed the model, it is possible to obtain model predictions using the `.predict()` method, which requires a feature array, including the same features and order as the training set, as input.*\n",
    "\n",
    "*Hint that isn't essential, but is worth thinking about - should the features be re-scaled in any way?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNNclf = KNeighborsClassifier( # complete\n",
    "preds = KNNclf.predict( # complete\n",
    "plt.figure()\n",
    "plt.scatter( # complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are almost identical to the training classifications. However, we have cheated! In this case we are evaluating the accuracy of the model (98% in this case) using the same data that defines the model. Thus, what we have really evaluated here is the training error. The relevant parameter, however, is the generalization error: how accurate are the model predictions on new data? \n",
    "\n",
    "Without going into too much detail, we will test this using cross validation (CV). In brief, CV provides predictions on the training set using a subset of the data to generate a model that predicts the class of the remaining sources. Using [`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_predict.html), we can get a better sense of the model accuracy. Predictions from `cross_val_predict` are produced in the following manner:\n",
    "\n",
    "    from sklearn.cross_validation import cross_val_predict\n",
    "    CVpreds = cross_val_predict(sklearn.model(), X, y)\n",
    "\n",
    "where `sklearn.model()` is the desired model, `X` is the feature array, and `y` is the label array.\n",
    "\n",
    "**Problem 3b** \n",
    "\n",
    "Produce cross-validation predictions for the iris dataset and a $k$NN with 5 neighbors. Plot the resulting classifications, as above, and estimate the accuracy of the model as applied to new data. How does this accuracy compare to a $k$NN with 50 neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "CVpreds = cross_val_predict( # complete\n",
    "\n",
    "plt.scatter( # complete\n",
    "print(\"The accuracy of the kNN = 5 model is ~{:.4}\".format( # complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is useful to understand the overall accuracy of the model, it is even more useful to understand the nature of the misclassifications that occur. \n",
    "\n",
    "**Problem 3c** \n",
    "\n",
    "Calculate the accuracy for each class in the iris set, as determined via CV for the $k$NN = 50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just found that the classifier does a much better job classifying setosa and versicolor than it does for virginica. The main reason for this is some viginica flowers lie far outside the main virginica locus, and within predominantly versicolor \"neighborhoods\". In addition to knowing the accuracy for the individual classes, it is also useful to know class predictions for the misclassified sources, or in other words where there is \"confusion\" for the classifier. The best way to summarize this information is with a confusion matrix. In a confusion matrix, one axis shows the true class and the other shows the predicted class. For a perfect classifier all of the power will be along the diagonal, while confusion is represented by off-diagonal signal. \n",
    "\n",
    "Like almost everything else we have encountered during this exercise, `scikit-learn` makes it easy to compute a confusion matrix. This can be accomplished with the following: \n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_prep)\n",
    "\n",
    "**Problem 3d** \n",
    "\n",
    "Calculate the confusion matrix for the iris training set and the $k$NN = 50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this representation, we see right away that most of the virginica that are being misclassifed are being scattered into the versicolor class. However, this representation could still be improved: it'd be helpful to normalize each value relative to the total number of sources in each class, and better still, it'd be good to have a visual representation of the confusion matrix. This visual representation will be readily digestible. Now let's normalize the confusion matrix.\n",
    "\n",
    "**Problem 3e** \n",
    "\n",
    "Calculate the normalized confusion matrix. Be careful, you have to sum along one axis, and then divide along the other. \n",
    "\n",
    "*Anti-hint: This operation is actually straightforward using some array manipulation that we have not covered up to this point. Thus, we have performed the necessary operations for you below. If you have extra time, you should try to develop an alternate way to arrive at the same normalization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_cm = cm.astype('float')/cm.sum(axis = 1)[:,np.newaxis]\n",
    "\n",
    "normalized_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization makes it easier to compare the classes, since each class has a different number of sources. Now we can procede with a visual representation of the confusion matrix. This is best done using `imshow()` within pyplot. You will also need to plot a colorbar, and labeling the axes will also be helpful. \n",
    "\n",
    "**Problem 3f** \n",
    "\n",
    "Plot the confusion matrix. Be sure to label each of the axeses.\n",
    "\n",
    "*Hint - you might find the [`sklearn` confusion matrix tutorial](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#example-model-selection-plot-confusion-matrix-py) helpful for making a nice plot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is straight-forward to see that virginica and versicolor flowers are the most likely to be confused, which we could intuit from the very first plot in this notebook, but this exercise becomes far more important for large data sets with many, many classes. \n",
    "\n",
    "Thus concludes our introduction to `scikit-learn` and supervised and unsupervised learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
